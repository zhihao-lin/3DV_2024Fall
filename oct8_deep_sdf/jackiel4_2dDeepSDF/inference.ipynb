{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code from DeepSDF https://github.com/facebookresearch/DeepSDF/tree/main\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "\n",
    "from torchinfo import summary\n",
    "from easydict import EasyDict as ed\n",
    "import time\n",
    "import random\n",
    "\n",
    "from model import SDFNet, Decoder, Simple\n",
    "import workspace as ws\n",
    "import os\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSDFDataset(Dataset):\n",
    "\tdef __init__(self, dataset):\n",
    "\t\tself.indices = range(len(dataset))\n",
    "\t\tself.sdf = dataset\n",
    "\t\tself.sampling_percent = 1\n",
    "\t\tself.sdf_dim = 28\n",
    "\t\tself.xy = self._init_xy()\n",
    "\n",
    "\tdef _init_xy(self):\n",
    "\t\tx = np.linspace(0,1,self.sdf_dim)\n",
    "\t\ty = x\n",
    "\t\txy = np.meshgrid(x, y)\n",
    "\t\txy = np.stack(xy).reshape(2,-1).T\n",
    "\t\treturn torch.tensor(xy, dtype=torch.float)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sdf)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tpos_data, pos_idx = self._get_pos_data()\n",
    "\t\tsdf, label = self.sdf[idx]\n",
    "\t\tsdf_data = self._get_s_data(sdf, pos_idx)[:, None]\n",
    "\t\t# print(pos_data.shape)\n",
    "\t\t# print(sdf_data.shape)\n",
    "\t\tX = torch.cat([pos_data, sdf_data], dim=-1)\n",
    "\t\treturn X, idx, label\n",
    "\t\n",
    "\tdef _get_pos_data(self):\n",
    "\t\tsample_size = math.ceil(self.sampling_percent * (self.sdf_dim**2))\n",
    "\t\t# get indices to index a position index.\n",
    "\t\t# pos_idx = random.sample(range(self.sdf_dim**2), sample_size)\n",
    "\t\tpos_idx = range(self.sdf_dim**2)\n",
    "\t\txy = self.xy[pos_idx]\n",
    "\t\treturn self.xy[pos_idx], pos_idx\n",
    "\t\n",
    "\tdef _get_s_data(self, sdf, pos_idx):\n",
    "\t\treturn -sdf.flatten()[pos_idx]\n",
    "\t\n",
    "\tdef get_pos_from_idx(self, idx_x, idx_y):\n",
    "\t\treturn self.xy[idx_y*self.sdf_dim + idx_x]\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)) #0.1307(0.3081,)\n",
    "        ])\n",
    "\n",
    "dataset1 = datasets.MNIST('./data', train=True,\n",
    "                    transform=transform)\n",
    "train_kwargs = {'batch_size': 6, 'shuffle':True}\n",
    "train_dataset = CustomSDFDataset(dataset1)\n",
    "sdf_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(\n",
    "    decoder,\n",
    "    num_iterations,\n",
    "    latent_size,\n",
    "    sdf_data,\n",
    "    stat,\n",
    "    clamp_dist,\n",
    "    num_samples=28*28,\n",
    "    lr=5e-4,\n",
    "    l2reg=False,\n",
    "):\n",
    "    def adjust_learning_rate(\n",
    "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "    ):\n",
    "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    decreased_by = 10\n",
    "    adjust_lr_every = int(num_iterations / 2)\n",
    "\n",
    "    if type(stat) == type(0.1):\n",
    "        latent = torch.ones(1, latent_size).normal_(mean=0, std=stat) #.cuda()\n",
    "    else:\n",
    "        latent = torch.normal(stat[0].detach(), stat[1].detach()) #.cuda()\n",
    "\n",
    "    latent.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam([latent], lr=lr)\n",
    "\n",
    "    loss_num = 0\n",
    "    loss_l1 = torch.nn.L1Loss()\n",
    "\n",
    "    for e in range(num_iterations):\n",
    "\n",
    "        decoder.eval()\n",
    "        xyz = sdf_data[:, 0:2]\n",
    "        sdf_gt = sdf_data[:, 2].unsqueeze(1)\n",
    "\n",
    "        # sdf_gt = torch.clamp(sdf_gt, -clamp_dist, clamp_dist)\n",
    "\n",
    "        adjust_learning_rate(lr, optimizer, e, decreased_by, adjust_lr_every)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        latent_inputs = latent.expand(num_samples, -1)\n",
    "\n",
    "        inputs = torch.cat([latent_inputs, xyz], 1) #.cuda()\n",
    "\n",
    "        pred_sdf = decoder(inputs)\n",
    "\n",
    "        # TODO: why is this needed?\n",
    "        if e == 0:\n",
    "            pred_sdf = decoder(inputs)\n",
    "\n",
    "        # pred_sdf = torch.clamp(pred_sdf, -clamp_dist, clamp_dist)\n",
    "\n",
    "        loss = loss_l1(pred_sdf, sdf_gt)\n",
    "        if l2reg:\n",
    "            loss += 1e-4 * torch.mean(latent.pow(2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            logging.debug(loss.cpu().data.numpy())\n",
    "            logging.debug(e)\n",
    "            logging.debug(latent.norm())\n",
    "        loss_num = loss.cpu().data.numpy()\n",
    "\n",
    "    return loss_num, latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_partial(\n",
    "    decoder,\n",
    "    num_iterations,\n",
    "    latent_size,\n",
    "    sdf_data,\n",
    "    stat,\n",
    "    clamp_dist,\n",
    "    num_samples=28*14,\n",
    "    lr=5e-4,\n",
    "    l2reg=False,\n",
    "):\n",
    "\tdef adjust_learning_rate(\n",
    "\t\tinitial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "\t):\n",
    "\t\tlr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n",
    "\t\tfor param_group in optimizer.param_groups:\n",
    "\t\t\tparam_group[\"lr\"] = lr\n",
    "\n",
    "\tdecreased_by = 10\n",
    "\tadjust_lr_every = int(num_iterations / 2)\n",
    "\n",
    "\tif type(stat) == type(0.1):\n",
    "\t\tlatent = torch.ones(1, latent_size).normal_(mean=0, std=stat) #.cuda()\n",
    "\telse:\n",
    "\t\tlatent = torch.normal(stat[0].detach(), stat[1].detach()) #.cuda()\n",
    "\n",
    "\tlatent.requires_grad = True\n",
    "\n",
    "\toptimizer = torch.optim.Adam([latent], lr=lr)\n",
    "\n",
    "\tloss_num = 0\n",
    "\tloss_l1 = torch.nn.L1Loss()\n",
    "\n",
    "\tsdf_data = sdf_data.view(28, 28, 3) # Shape to image dimensions\n",
    "\n",
    "\tsdf_data = sdf_data[:, :14, :].reshape(-1, 3) # keep only right half of image\n",
    "\n",
    "\tfor e in range(num_iterations):\n",
    "\n",
    "\t\tdecoder.eval()\n",
    "\t\t\n",
    "\t\txyz = sdf_data[:, 0:2]\n",
    "\t\tsdf_gt = sdf_data[:, 2].unsqueeze(1)\n",
    "\n",
    "\t\t# sdf_gt = torch.clamp(sdf_gt, -clamp_dist, clamp_dist)\n",
    "\n",
    "\t\tadjust_learning_rate(lr, optimizer, e, decreased_by, adjust_lr_every)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tlatent_inputs = latent.expand(num_samples, -1)\n",
    "\n",
    "\t\tinputs = torch.cat([latent_inputs, xyz], 1) #.cuda()\n",
    "\n",
    "\t\tpred_sdf = decoder(inputs)\n",
    "\n",
    "\t\t# TODO: why is this needed?\n",
    "\t\tif e == 0:\n",
    "\t\t\tpred_sdf = decoder(inputs)\n",
    "\n",
    "\t\t# pred_sdf = torch.clamp(pred_sdf, -clamp_dist, clamp_dist)\n",
    "\n",
    "\t\tloss = loss_l1(pred_sdf, sdf_gt)\n",
    "\t\tif l2reg:\n",
    "\t\t\tloss += 1e-4 * torch.mean(latent.pow(2))\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif e % 50 == 0:\n",
    "\t\t\tlogging.debug(loss.cpu().data.numpy())\n",
    "\t\t\tlogging.debug(e)\n",
    "\t\t\tlogging.debug(latent.norm())\n",
    "\t\tloss_num = loss.cpu().data.numpy()\n",
    "\n",
    "\treturn loss_num, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrschedule = [\n",
    "\t{ # Lr schedule for decoder\n",
    "\t\t\"Type\": \"Step\",\n",
    "\t\t\"Initial\":1e-4,\n",
    "\t\t\"Interval\":300, #Step at every nth epoch\n",
    "\t\t\"Factor\":0.5\n",
    "\t},\n",
    "\t{ # lr schedule for embeddings\n",
    "\t\t\"Type\": \"Step\",\n",
    "\t\t\"Initial\":3e-4,\n",
    "\t\t\"Interval\":300,\n",
    "\t\t\"Factor\":0.5,\n",
    "\t}\n",
    "]\n",
    "\n",
    "config = {\n",
    "\t'latent_size': 3,\n",
    "\t'code_bound': 1,\n",
    "\t'code_regularization': True, #False, #True,\n",
    "\t\"CodeRegularizationLambda\": 1e-4,\n",
    "\t'CodeInitStdDev': 1.0,\n",
    "\t'LearningRateSchedule': lrschedule,\n",
    "\t'grad_clip': None, #2.0, # None, # 1.0, # None, # for decoder\n",
    "\t\"SnapshotFrequency\": 30, # checkpoints\n",
    "\t\"NumEpochs\": 2000,\n",
    "\t\"LogFrequency\": 5,\n",
    "\t\"ClampingDistance\": 0.3,\n",
    "\t\"NetworkSpecs\" : {\n",
    "\t\t\"dims\" : [ 512, 512, 512, 512, 512, 512, 512, 512 ],\n",
    "\t\t\"dropout\" : [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "\t\t\"dropout_prob\" : 0.2,\n",
    "\t\t\"norm_layers\" : [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "\t\t\"latent_in\" : [4],\n",
    "\t\t\"xyz_in_all\" : False,\n",
    "\t\t\"use_tanh\" : False,\n",
    "\t\t\"latent_dropout\" : False,\n",
    "\t\t\"weight_norm\" : True\n",
    "\t\t},\n",
    "}\n",
    "\n",
    "config = ed(config)\n",
    "experiment_directory = os.path.join('./experiment')\n",
    "\n",
    "args = ed({\n",
    "\t'experiment_directory': './experiment',\n",
    "\t'checkpoint': 'latest',\n",
    "\t'iterations': 800,\n",
    "\t'split_filename': 'the split to reconstruct',\n",
    "\n",
    "})\n",
    "\n",
    "def empirical_stat(latent_vecs, indices):\n",
    "\tlat_mat = torch.zeros(0) #.cuda()\n",
    "\tfor ind in indices:\n",
    "\t\tlat_mat = torch.cat([lat_mat, latent_vecs[ind]], 0)\n",
    "\tmean = torch.mean(lat_mat, 0)\n",
    "\tvar = torch.var(lat_mat, 0)\n",
    "\treturn mean, var\n",
    "\n",
    "latent_size = config[\"latent_size\"]\n",
    "\n",
    "decoder = SDFNet(latent_size)\n",
    "\n",
    "saved_model_state = torch.load(\n",
    "\tos.path.join(\n",
    "\t\targs.experiment_directory, ws.model_params_subdir, args.checkpoint + \".pth\"\n",
    "\t)\n",
    ")\n",
    "saved_model_epoch = saved_model_state[\"epoch\"]\n",
    "\n",
    "decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n",
    "\n",
    "logging.debug(decoder)\n",
    "\n",
    "err_sum = 0.0\n",
    "repeat = 1\n",
    "save_latvec_only = False\n",
    "rerun = 0\n",
    "\n",
    "reconstruction_dir = os.path.join(\n",
    "\targs.experiment_directory, ws.reconstructions_subdir, str(saved_model_epoch)\n",
    ")\n",
    "\n",
    "if not os.path.isdir(reconstruction_dir):\n",
    "\tos.makedirs(reconstruction_dir)\n",
    "\n",
    "reconstruction_meshes_dir = os.path.join(\n",
    "\treconstruction_dir, ws.reconstruction_meshes_subdir\n",
    ")\n",
    "reconstruction_partial_dir = os.path.join(\n",
    "\treconstruction_dir, \"Partial\"\n",
    ")\n",
    "if not os.path.isdir(reconstruction_meshes_dir):\n",
    "\tos.makedirs(reconstruction_meshes_dir)\n",
    "if not os.path.isdir(reconstruction_partial_dir):\n",
    "\tos.makedirs(reconstruction_partial_dir)\n",
    "reconstruction_codes_dir = os.path.join(\n",
    "\treconstruction_dir, ws.reconstruction_codes_subdir\n",
    ")\n",
    "if not os.path.isdir(reconstruction_codes_dir):\n",
    "\tos.makedirs(reconstruction_codes_dir)\n",
    "\n",
    "\n",
    "def create_image(decoder, wh, sdf_gt, lat_vec, i):\n",
    "    \n",
    "\t# make grid\n",
    "\tx = y = np.linspace(0,1.0,wh)\n",
    "\txy = np.meshgrid(x, y)\n",
    "\txy = torch.tensor(np.stack(xy).reshape(2,-1).T, dtype=torch.float)\n",
    "\n",
    "\tinput = torch.cat([lat_vec.repeat(len(xy), 1), xy],-1)\n",
    "\tS = decoder(input)\n",
    "\tS = S.view(wh,-1)\n",
    "\n",
    "\tfig1 = plt.figure()\n",
    "\tc = plt.gca().imshow(S.detach().numpy(), cmap='gray')\n",
    "\tfilename = os.path.join(reconstruction_meshes_dir, \"SDF-\" + str(i) + \"inference.png\")\n",
    "\tplt.savefig(filename)\n",
    "\n",
    "\tx_ = sdf_gt[:,0]\n",
    "\ty_ = sdf_gt[:,1]\n",
    "\tsdf_ = sdf_gt[:,2]\n",
    "\tsdf_gt_blah = sdf_gt[:,2].view(28,-1)\n",
    "\tfig2 = plt.figure()\n",
    "\tc = plt.gca().imshow(sdf_gt_blah.detach().numpy(), cmap='gray')\n",
    "\t# c = plt.scatter(x_, y_, c=sdf_)\n",
    "\tfilename = os.path.join(reconstruction_meshes_dir, \"SDF-\" + str(i) + \"gt.png\")\n",
    "\tplt.savefig(filename)\n",
    "\n",
    "def create_image_partial(decoder, wh, sdf_gt, lat_vec, i):\n",
    "    \n",
    "\t# make grid\n",
    "\tx = y = np.linspace(0,1.0,wh)\n",
    "\txy = np.meshgrid(x, y)\n",
    "\txy = torch.tensor(np.stack(xy).reshape(2,-1).T, dtype=torch.float)\n",
    "\n",
    "\tinput = torch.cat([lat_vec.repeat(len(xy), 1), xy],-1)\n",
    "\tS = decoder(input)\n",
    "\tS = S.view(wh,-1)\n",
    "\n",
    "\tfig1 = plt.figure()\n",
    "\tc = plt.gca().imshow(S.detach().numpy(), cmap='gray')\n",
    "\tfilename = os.path.join(reconstruction_partial_dir, \"SDF-\" + str(i) + \"inference.png\")\n",
    "\tplt.savefig(filename)\n",
    "\n",
    "\tx_ = sdf_gt[:,0]\n",
    "\ty_ = sdf_gt[:,1]\n",
    "\tsdf_ = sdf_gt[:,2]\n",
    "\tsdf_gt = sdf_gt[:,2].view(28,-1)\n",
    "\tfig2 = plt.figure()\n",
    "\tc = plt.gca().imshow(sdf_gt.detach().numpy(), cmap='gray')\n",
    "\t# c = plt.scatter(x_, y_, c=sdf_)\n",
    "\tfilename = os.path.join(reconstruction_partial_dir, \"SDF-\" + str(i) + \"gt.png\")\n",
    "\tplt.savefig(filename)\n",
    "\n",
    "\t# sdf_gt = sdf_gt[:,2].view(28,-1)\n",
    "\tsdf_gt[:, :14] = 0\n",
    "\tfig2 = plt.figure()\n",
    "\tc = plt.gca().imshow(sdf_gt.detach().numpy(), cmap='gray')\n",
    "\t# c = plt.scatter(x_, y_, c=sdf_)\n",
    "\tfilename = os.path.join(reconstruction_partial_dir, \"SDF-\" + str(i) + \"gt_partial.png\")\n",
    "\tplt.savefig(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reconstruct 1. Low resolution unseen images and 2. Low resolution partial images\n",
    "data_sdf, _, label = next(iter(sdf_loader))\n",
    "start = time.time()\n",
    "\n",
    "for i, sdf in enumerate(data_sdf):\n",
    "\terr, latent = reconstruct(\n",
    "\t\tdecoder,\n",
    "\t\tint(args.iterations),\n",
    "\t\tlatent_size,\n",
    "\t\tsdf,\n",
    "\t\t0.01,  # [emp_mean,emp_var],\n",
    "\t\t0.1,\n",
    "\t\tnum_samples=28*28,\n",
    "\t\tlr=5e-3,\n",
    "\t\tl2reg=True,\n",
    "\t)\n",
    "\tlogging.debug(\"reconstruct time: {}\".format(time.time() - start))\n",
    "\terr_sum += err\n",
    "\t# logging.debug(\"current_error avg: {}\".format((err_sum / (ii + 1))))\n",
    "\t# logging.debug(ii)\n",
    "\n",
    "\t# logging.debug(\"latent: {}\".format(latent.detach().cpu().numpy()))\n",
    "\n",
    "\tdecoder.eval()\n",
    "\n",
    "\n",
    "\t# sdf_gt = sdf[:, 2].unsqueeze(1)\n",
    "\tsdf_gt = sdf\n",
    "\twh = 1000\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tcreate_image(decoder, wh, sdf_gt, latent, i)\n",
    "\n",
    "for i, sdf in enumerate(data_sdf):\n",
    "\terr, latent = reconstruct_partial(\n",
    "\t\tdecoder,\n",
    "\t\tint(args.iterations),\n",
    "\t\tlatent_size,\n",
    "\t\tsdf,\n",
    "\t\t0.01,  # [emp_mean,emp_var],\n",
    "\t\t0.1,\n",
    "\t\tnum_samples=28*14,\n",
    "\t\tlr=5e-3,\n",
    "\t\tl2reg=True,\n",
    "\t)\n",
    "\tlogging.debug(\"reconstruct time: {}\".format(time.time() - start))\n",
    "\terr_sum += err\n",
    "\t# logging.debug(\"current_error avg: {}\".format((err_sum / (ii + 1))))\n",
    "\t# logging.debug(ii)\n",
    "\n",
    "\t# logging.debug(\"latent: {}\".format(latent.detach().cpu().numpy()))\n",
    "\n",
    "\tdecoder.eval()\n",
    "\t# sdf_gt = sdf[:, 2].unsqueeze(1)\n",
    "\tsdf_gt = sdf\n",
    "\twh = 1000\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tprint('create partial')\n",
    "\t\tcreate_image_partial(decoder, wh, sdf_gt, latent, i)\n",
    "\t\n",
    "\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
